"""
GRC Compliance Mapping - Backend API avec ML/NLP
FastAPI + PostgreSQL + Sentence-Transformers
"""

from fastapi import FastAPI, UploadFile, File, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session
from typing import List, Optional
import pandas as pd
from loguru import logger
import os

from database import get_db, engine, Base
from models import Requirement, SCFControl, ComplianceMapping, ImportSession
from ml_service import MLMappingService
from schemas import (
    RequirementCreate,
    RequirementResponse,
    MappingCreate,
    MappingResponse,
    BulkImportResponse,
    SimilaritySearchRequest,
    SimilaritySearchResponse
)

# Cr√©er les tables
Base.metadata.create_all(bind=engine)

# Initialiser FastAPI
app = FastAPI(
    title="GRC Compliance Mapping API",
    description="API pour le mapping intelligent de conformit√© avec ML/NLP",
    version="1.0.0"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:3001",  # Frontend Docker
        "http://localhost:3002",
        "http://localhost:3003",  # Frontend Dev (alternate port)
        "http://localhost:5173"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialiser le service ML
ml_service = MLMappingService()

# Imports pour SCF (ne pas charger tout de suite)
from scf_knowledge_service import get_scf_knowledge_base
from scf_api_routes import router as scf_router

# Variable globale pour la base SCF (chargement lazy)
scf_kb = None
scf_kb_initialized = False

def get_or_init_scf_kb():
    """R√©cup√®re la base SCF en la chargeant si n√©cessaire (lazy loading)"""
    global scf_kb, scf_kb_initialized

    if scf_kb_initialized:
        return scf_kb

    try:
        logger.info("üìö Initialisation de la base de connaissances SCF (lazy loading)...")
        scf_kb = get_scf_knowledge_base()
        # Initialiser les embeddings avec le mod√®le ML
        scf_kb.init_semantic_model(ml_service.model)
        scf_kb_initialized = True
        logger.info("‚úÖ Base SCF pr√™te et index√©e")
        return scf_kb
    except Exception as e:
        logger.error(f"‚ùå Erreur initialisation SCF: {e}")
        scf_kb_initialized = True  # Marquer comme tent√© pour √©viter les retry
        return None

# Inclure les routes SCF
app.include_router(scf_router)

# ============================================
# Health Check
# ============================================

@app.get("/")
async def root():
    """Point d'entr√©e de l'API"""
    return {
        "message": "GRC Compliance Mapping API",
        "version": "1.0.0",
        "status": "running",
        "ml_model": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    }

@app.get("/health")
async def health_check():
    """V√©rification de sant√© de l'API"""
    return {
        "status": "healthy",
        "database": "connected",
        "ml_service": "ready"
    }

# ============================================
# Import Excel
# ============================================

@app.post("/api/import/excel", response_model=BulkImportResponse)
async def import_excel(
    file: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """
    Importer un fichier Excel (sans analyse IA)
    Les donn√©es sont directement ins√©r√©es dans PostgreSQL
    Cr√©e automatiquement une ImportSession pour tracer l'import
    """
    try:
        logger.info(f"D√©but de l'import du fichier: {file.filename}")

        # Lire le fichier Excel
        contents = await file.read()

        # D√©tecter toutes les feuilles
        excel_file = pd.ExcelFile(contents)
        logger.info(f"Feuilles d√©tect√©es: {excel_file.sheet_names}")

        # Cr√©er une session d'import
        import_session = ImportSession(
            filename=file.filename,
            source_sheet=", ".join(excel_file.sheet_names),
            status='processing',
            analysis_source='pending',
            total_requirements=0
        )
        db.add(import_session)
        db.flush()  # Pour obtenir l'ID

        logger.info(f"Session d'import cr√©√©e: ID={import_session.id}")

        total_imported = 0
        imported_sheets = []
        
        # Importer chaque feuille
        for sheet_name in excel_file.sheet_names:
            df = pd.read_excel(contents, sheet_name=sheet_name)
            
            logger.info(f"Traitement de la feuille '{sheet_name}': {len(df)} lignes")
            
            # D√©tecter les colonnes importantes
            columns = df.columns.tolist()
            
            # Mapper les colonnes (flexible)
            id_col = next((col for col in columns if any(x in col.lower() for x in ['id', 'n¬∞', 'numero'])), None)
            req_col = next((col for col in columns if any(x in col.lower() for x in ['exigence', 'requirement', 'control', 'titre'])), None)
            verif_col = next((col for col in columns if any(x in col.lower() for x in ['verification', 'point', 'description'])), None)
            
            if not req_col:
                logger.warning(f"Aucune colonne d'exigence trouv√©e dans '{sheet_name}', skip")
                continue
            
            # Ins√©rer les donn√©es
            for idx, row in df.iterrows():
                try:
                    original_id_val = str(row[id_col]) if id_col and pd.notna(row[id_col]) else f"{sheet_name}_{idx}"

                    # V√©rifier si l'exigence existe d√©j√† (duplicate key)
                    existing = db.query(Requirement).filter(
                        Requirement.original_id == original_id_val,
                        Requirement.source_file == file.filename
                    ).first()

                    if existing:
                        # Skip silencieusement les doublons
                        logger.debug(f"Skip duplicate: {original_id_val} from {file.filename}")
                        continue

                    requirement = Requirement(
                        original_id=original_id_val,
                        requirement=str(row[req_col]) if pd.notna(row[req_col]) else "",
                        verification_point=str(row[verif_col]) if verif_col and pd.notna(row[verif_col]) else None,
                        source_file=file.filename,
                        source_sheet=sheet_name,
                        analysis_status='pending',
                        import_session_id=import_session.id  # Lier √† la session d'import
                    )

                    db.add(requirement)
                    total_imported += 1

                except Exception as e:
                    logger.error(f"Erreur ligne {idx}: {e}")
                    continue
            
            db.commit()
            imported_sheets.append({
                "sheet_name": sheet_name,
                "rows_imported": len(df)
            })

        # Mettre √† jour la session d'import
        import_session.total_requirements = total_imported
        import_session.status = 'completed' if total_imported > 0 else 'failed'
        db.commit()

        logger.info(f"Import termin√©: {total_imported} lignes import√©es (Session ID: {import_session.id})")

        return BulkImportResponse(
            success=True,
            total_imported=total_imported,
            sheets=imported_sheets,
            message=f"Import r√©ussi: {total_imported} exigences import√©es",
            import_session_id=import_session.id  # Retourner l'ID de la session
        )
        
    except Exception as e:
        logger.error(f"Erreur lors de l'import: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================
# CRUD Requirements
# ============================================

@app.get("/api/requirements", response_model=List[RequirementResponse])
async def get_requirements(
    skip: int = 0,
    limit: int = 100,
    status: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """R√©cup√©rer la liste des exigences"""
    query = db.query(Requirement)
    
    if status:
        query = query.filter(Requirement.analysis_status == status)
    
    requirements = query.offset(skip).limit(limit).all()
    return requirements

@app.get("/api/requirements/{requirement_id}", response_model=RequirementResponse)
async def get_requirement(requirement_id: int, db: Session = Depends(get_db)):
    """R√©cup√©rer une exigence sp√©cifique"""
    requirement = db.query(Requirement).filter(Requirement.id == requirement_id).first()
    
    if not requirement:
        raise HTTPException(status_code=404, detail="Exigence non trouv√©e")
    
    return requirement

@app.post("/api/requirements", response_model=RequirementResponse)
async def create_requirement(
    requirement: RequirementCreate,
    db: Session = Depends(get_db)
):
    """Cr√©er une nouvelle exigence manuellement"""
    db_requirement = Requirement(**requirement.dict())
    db.add(db_requirement)
    db.commit()
    db.refresh(db_requirement)
    return db_requirement

@app.put("/api/requirements/{requirement_id}", response_model=RequirementResponse)
async def update_requirement(
    requirement_id: int,
    requirement: RequirementCreate,
    db: Session = Depends(get_db)
):
    """Mettre √† jour une exigence"""
    db_requirement = db.query(Requirement).filter(Requirement.id == requirement_id).first()
    
    if not db_requirement:
        raise HTTPException(status_code=404, detail="Exigence non trouv√©e")
    
    for key, value in requirement.dict().items():
        setattr(db_requirement, key, value)
    
    db.commit()
    db.refresh(db_requirement)
    return db_requirement

@app.delete("/api/requirements/{requirement_id}")
async def delete_requirement(requirement_id: int, db: Session = Depends(get_db)):
    """Supprimer une exigence"""
    db_requirement = db.query(Requirement).filter(Requirement.id == requirement_id).first()
    
    if not db_requirement:
        raise HTTPException(status_code=404, detail="Exigence non trouv√©e")
    
    db.delete(db_requirement)
    db.commit()
    return {"message": "Exigence supprim√©e"}

# ============================================
# ML-Powered Mapping
# ============================================

@app.post("/api/analyze/similarity", response_model=List[SimilaritySearchResponse])
async def find_similar_controls(
    request: SimilaritySearchRequest,
    db: Session = Depends(get_db)
):
    """
    Trouver les contr√¥les SCF les plus similaires √† une exigence
    Utilise Sentence-Transformers pour la similarit√© s√©mantique
    """
    try:
        # R√©cup√©rer tous les contr√¥les SCF
        scf_controls = db.query(SCFControl).all()
        
        if not scf_controls:
            raise HTTPException(status_code=404, detail="Aucun contr√¥le SCF trouv√© dans la base")
        
        # Utiliser le service ML pour trouver les similarit√©s
        results = ml_service.find_similar_controls(
            requirement_text=request.requirement_text,
            controls=scf_controls,
            top_k=request.top_k or 5
        )
        
        return results
        
    except Exception as e:
        logger.error(f"Erreur lors de la recherche de similarit√©: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/analyze/batch")
async def analyze_batch(
    requirement_ids: List[int],
    db: Session = Depends(get_db)
):
    """
    Analyser un lot d'exigences avec ML
    G√©n√®re automatiquement les mappings sugg√©r√©s
    """
    try:
        results = []
        
        for req_id in requirement_ids:
            requirement = db.query(Requirement).filter(Requirement.id == req_id).first()
            
            if not requirement:
                continue
            
            # Trouver les contr√¥les similaires
            scf_controls = db.query(SCFControl).all()
            similar = ml_service.find_similar_controls(
                requirement_text=requirement.requirement,
                controls=scf_controls,
                top_k=3
            )
            
            if similar:
                # Cr√©er un mapping avec le meilleur match
                best_match = similar[0]
                
                mapping = ComplianceMapping(
                    requirement_id=req_id,
                    scf_mapping=f"{best_match.control_id} - {best_match.control_title}",
                    confidence_score=best_match.similarity_score,
                    mapping_source='ml',
                    analysis=f"Mapping automatique ML (similarit√©: {best_match.similarity_score:.2%})"
                )
                
                db.add(mapping)
                requirement.analysis_status = 'analyzed'
                
                results.append({
                    "requirement_id": req_id,
                    "best_match": best_match.dict(),
                    "status": "success"
                })
        
        db.commit()
        
        return {
            "success": True,
            "analyzed_count": len(results),
            "results": results
        }
        
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse batch: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================
# Save Claude Results
# ============================================

@app.post("/api/save-claude-results")
async def save_claude_results(
    data: dict,
    db: Session = Depends(get_db)
):
    """
    Sauvegarder les r√©sultats Claude depuis le navigateur
    Cr√©e ou utilise une session d'import existante
    """
    try:
        results = data.get('results', [])
        import_session_id = data.get('import_session_id')
        filename = data.get('filename', 'claude_import')

        # Cr√©er ou r√©cup√©rer la session d'import
        if import_session_id:
            import_session = db.query(ImportSession).filter(ImportSession.id == import_session_id).first()
            if not import_session:
                raise HTTPException(status_code=404, detail=f"Session {import_session_id} introuvable")
        else:
            # Cr√©er une nouvelle session
            import_session = ImportSession(
                filename=filename,
                source_sheet='Claude Results',
                status='processing',
                analysis_source='claude',
                total_requirements=len(results)
            )
            db.add(import_session)
            db.flush()

        saved_count = 0
        skipped_count = 0

        for result in results:
            try:
                original_id_val = result.get('id', '')

                # V√©rifier si l'exigence existe d√©j√†
                existing = db.query(Requirement).filter(
                    Requirement.original_id == original_id_val,
                    Requirement.source_file == filename
                ).first()

                if existing:
                    # Skip silencieusement les doublons
                    logger.debug(f"Skip duplicate requirement: {original_id_val} from {filename}")
                    skipped_count += 1
                    continue

                # Cr√©er l'exigence
                requirement = Requirement(
                    original_id=original_id_val,
                    requirement=result.get('requirement', ''),
                    verification_point=result.get('verificationPoint'),
                    source_file=filename,
                    source_sheet='Claude Results',
                    analysis_status='analyzed',
                    import_session_id=import_session.id
                )

                db.add(requirement)
                db.flush()  # Pour obtenir l'ID

                # Cr√©er le mapping
                mapping = ComplianceMapping(
                    requirement_id=requirement.id,
                    scf_mapping=result.get('scfMapping'),
                    iso27001_mapping=result.get('iso27001Mapping'),
                    iso27002_mapping=result.get('iso27002Mapping'),
                    cobit5_mapping=result.get('cobit5Mapping'),
                    confidence_score=0.95,  # Claude = haute confiance
                    mapping_source='claude',
                    analysis=result.get('analysis', ''),
                    import_session_id=import_session.id
                )

                db.add(mapping)
                saved_count += 1

            except Exception as e:
                logger.error(f"Erreur lors de la sauvegarde de l'exigence {result.get('id')}: {e}")
                continue

        # Mettre √† jour la session
        import_session.status = 'completed'
        import_session.total_requirements = saved_count
        db.commit()

        logger.info(f"OK - {saved_count} resultats Claude sauvegardes, {skipped_count} doublons skipp√©s (Session ID: {import_session.id})")

        message = f"{saved_count} r√©sultats sauvegard√©s dans PostgreSQL"
        if skipped_count > 0:
            message += f" ({skipped_count} doublons ignor√©s)"

        return {
            "success": True,
            "saved_count": saved_count,
            "skipped_count": skipped_count,
            "import_session_id": import_session.id,
            "message": message
        }

    except Exception as e:
        db.rollback()
        logger.error(f"Erreur lors de la sauvegarde: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================
# Import Sessions Management
# ============================================

@app.get("/api/import-sessions")
async def get_import_sessions(
    limit: int = 50,
    offset: int = 0,
    status: Optional[str] = None,
    analysis_source: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """
    R√©cup√©rer la liste des imports pass√©s avec filtres
    """
    try:
        query = db.query(ImportSession)

        # Filtres
        if status:
            query = query.filter(ImportSession.status == status)
        if analysis_source:
            query = query.filter(ImportSession.analysis_source == analysis_source)

        # Tri par date d√©croissante (plus r√©cent en premier)
        query = query.order_by(ImportSession.import_date.desc())

        # Pagination
        total = query.count()
        sessions = query.offset(offset).limit(limit).all()

        return {
            "success": True,
            "total": total,
            "sessions": [
                {
                    "id": s.id,
                    "filename": s.filename,
                    "source_sheet": s.source_sheet,
                    "import_date": s.import_date.isoformat() if s.import_date else None,
                    "total_requirements": s.total_requirements,
                    "analysis_source": s.analysis_source,
                    "status": s.status,
                    "tags": s.tags,
                    "metadata": s.session_metadata
                }
                for s in sessions
            ]
        }

    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des sessions: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/import-sessions/{session_id}/results")
async def get_import_session_results(
    session_id: int,
    db: Session = Depends(get_db)
):
    """
    Charger les r√©sultats d'un import sp√©cifique
    """
    try:
        # V√©rifier que la session existe
        session = db.query(ImportSession).filter(ImportSession.id == session_id).first()
        if not session:
            raise HTTPException(status_code=404, detail=f"Session {session_id} introuvable")

        # R√©cup√©rer toutes les exigences de cette session avec leurs mappings
        requirements = db.query(Requirement).filter(
            Requirement.import_session_id == session_id
        ).all()

        # Formater les r√©sultats
        results = []
        for req in requirements:
            # R√©cup√©rer le mapping actif
            mapping = db.query(ComplianceMapping).filter(
                ComplianceMapping.requirement_id == req.id,
                ComplianceMapping.is_active == True,
                ComplianceMapping.import_session_id == session_id
            ).first()

            result = {
                "id": req.original_id or str(req.id),
                "requirement": req.requirement,
                "verificationPoint": req.verification_point,
                "scfMapping": mapping.scf_mapping if mapping else None,
                "iso27001Mapping": mapping.iso27001_mapping if mapping else None,
                "iso27002Mapping": mapping.iso27002_mapping if mapping else None,
                "cobit5Mapping": mapping.cobit5_mapping if mapping else None,
                "analysis": mapping.analysis if mapping else None,
                "confidenceScore": float(mapping.confidence_score) if mapping and mapping.confidence_score else None,
                "mappingSource": mapping.mapping_source if mapping else None
            }
            results.append(result)

        return {
            "success": True,
            "session": {
                "id": session.id,
                "filename": session.filename,
                "source_sheet": session.source_sheet,
                "import_date": session.import_date.isoformat() if session.import_date else None,
                "total_requirements": session.total_requirements,
                "analysis_source": session.analysis_source,
                "status": session.status,
                "tags": session.tags
            },
            "results": results
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors du chargement des r√©sultats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================
# Statistics
# ============================================

@app.get("/api/stats")
async def get_statistics(db: Session = Depends(get_db)):
    """R√©cup√©rer les statistiques globales"""
    total_requirements = db.query(Requirement).count()
    analyzed = db.query(Requirement).filter(Requirement.analysis_status == 'analyzed').count()
    pending = db.query(Requirement).filter(Requirement.analysis_status == 'pending').count()
    manual = db.query(Requirement).filter(Requirement.analysis_status == 'manual').count()
    
    total_mappings = db.query(ComplianceMapping).filter(ComplianceMapping.is_active == True).count()
    
    return {
        "total_requirements": total_requirements,
        "analyzed": analyzed,
        "pending": pending,
        "manual": manual,
        "total_mappings": total_mappings,
        "completion_rate": (analyzed + manual) / total_requirements * 100 if total_requirements > 0 else 0
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)

